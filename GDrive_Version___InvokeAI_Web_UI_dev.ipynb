{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AiDarkEzio/Project-Jupyter-NoteBooks/blob/main/GDrive_Version___InvokeAI_Web_UI_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pupm5-S2l3CQ"
      },
      "outputs": [],
      "source": [
        "#@markdown # Installing InvokeAI\n",
        "\n",
        "#@markdown Use Google Drive to store models (uses about 13 GB). Uncheck this if you don't have enough space in your Drive.\n",
        "useGoogleDrive = True #@param {type:\"boolean\"}\n",
        "\n",
        "googleDriveModelsFolder = '/stablemodels' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown This step usually takes about 5 minutes.\n",
        "\n",
        "#@markdown You can ignore the message about restarting the runtime.\n",
        "import os\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "if useGoogleDrive:\n",
        "  drive.mount('/content/drive')\n",
        "  if not googleDriveModelsFolder.startswith('/'):\n",
        "    googleDriveModelsFolder = '/' + googleDriveModelsFolder\n",
        "  modelsPath = \"/content/drive/MyDrive\"+googleDriveModelsFolder\n",
        "  if not modelsPath.endswith(\"/\"):\n",
        "   modelsPath = modelsPath + \"/\"\n",
        "\n",
        "env = os.environ.copy()\n",
        "\n",
        "!git clone -b main https://github.com/invoke-ai/InvokeAI\n",
        "#!git clone --depth 1 --branch 3.0.2rc1 https://github.com/invoke-ai/InvokeAI\n",
        "\n",
        "!pip install pillow==9.5.0 requests==2.31.0 xformers==0.0.20 triton==2.0.0\n",
        "!pip install git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "!pip install git+https://github.com/Birch-san/k-diffusion.git@mps#egg=k-diffusion\n",
        "!pip install git+https://github.com/invoke-ai/clipseg.git@relaxed-python-requirement#egg=clipseg\n",
        "!pip install git+https://github.com/invoke-ai/PyPatchMatch@0.1.4#egg=pypatchmatch\n",
        "%cd /content/InvokeAI/\n",
        "!pip install -q -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/InvokeAI/invokeai\n",
        "!mkdir /content/InvokeAI/invokeai/configs\n",
        "\n",
        "#@markdown Download only the default model in initial configuration.\n",
        "#@markdown Checking this prevents running out of space in Colab.\n",
        "\n",
        "defaultOnly = True #@param {type:\"boolean\"}\n",
        "skipWeights = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown This step usually takes about 2 minutes with only the default model and no weights.\n",
        "\n",
        "#@markdown You can ignore \"File exists\" warnings in the output.\n",
        "\n",
        "cmd = 'invokeai-configure --root_dir /content/InvokeAI/invokeai --yes'\n",
        "\n",
        "if defaultOnly:\n",
        "  cmd += ' --default_only'\n",
        "\n",
        "if skipWeights:\n",
        "  cmd += ' --skip-sd-weights'\n",
        "\n",
        "subprocess.run(cmd, shell=True, env=env)"
      ],
      "metadata": {
        "id": "Xxd5ULHus6d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Adding the SDXL Base Model\n",
        "\n",
        "#@markdown Installing SDXL base took about 20 minutes initially, but it's finished instantly\n",
        "#@markdown in subsequent runs if Google Drive is enabled. You can execute the first run in a runtime\n",
        "#@markdown without a GPU to save the model to Google Drive without spending GPU time.\n",
        "\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "# Install the SDXL base model\n",
        "def installSdxl(env):\n",
        "  installCmd = 'invokeai-model-install --add \"stabilityai/stable-diffusion-xl-base-1.0\" --root_dir /content/InvokeAI/invokeai'\n",
        "  subprocess.run(installCmd, shell=True, env=env)\n",
        "\n",
        "sdxlBaseSubfolderName = '/stable-diffusion-xl-base-1-0'\n",
        "\n",
        "if useGoogleDrive:\n",
        "  alreadyInstalled = True\n",
        "\n",
        "  driveSdxlMainFolder = modelsPath + \"sdxl/main\"\n",
        "  if not path.exists(driveSdxlMainFolder):\n",
        "    os.makedirs(driveSdxlMainFolder, exist_ok=True)\n",
        "    alreadyInstalled = False\n",
        "\n",
        "  localModelsSdxlFolder = \"/content/InvokeAI/invokeai/models/sdxl/\"\n",
        "  localSdxlMainFolder = localModelsSdxlFolder + \"main\"\n",
        "\n",
        "  subprocess.run('rm -rf ' + localModelsSdxlFolder, shell=True, env=env)\n",
        "  subprocess.run('rmdir ' + localModelsSdxlFolder, shell=True, env=env)\n",
        "\n",
        "  if not alreadyInstalled:\n",
        "    if not path.exists(localModelsSdxlFolder):\n",
        "      os.makedirs(localModelsSdxlFolder, exist_ok=True)\n",
        "    subprocess.run('ln -s '+driveSdxlMainFolder+' '+localModelsSdxlFolder, shell=True, env=env)\n",
        "    installSdxl(env)\n",
        "  else:\n",
        "    if not path.exists(localSdxlMainFolder):\n",
        "      os.makedirs(localSdxlMainFolder, exist_ok=True)\n",
        "    subprocess.run('ln -s '+driveSdxlMainFolder + sdxlBaseSubfolderName+' '+ localSdxlMainFolder, shell=True, env=env)\n",
        "    updateModelsYaml = True\n",
        "    with open('/content/InvokeAI/invokeai/configs/models.yaml') as f:\n",
        "      if 'stable-diffusion-xl-base-1-0' in f.read():\n",
        "        updateModelsYaml = False\n",
        "    if updateModelsYaml:\n",
        "      with open('/content/InvokeAI/invokeai/configs/models.yaml', 'a') as file:\n",
        "        lines = [\n",
        "          'sdxl/main/stable-diffusion-xl-base-1-0:\\n',\n",
        "          '  path: sdxl/main/stable-diffusion-xl-base-1-0\\n',\n",
        "          '  description: Stable Diffusion XL base model (12 GB)\\n',\n",
        "          '  variant: normal\\n',\n",
        "          '  format: diffusers\\n'\n",
        "        ]\n",
        "        file.writelines(lines)\n",
        "else:\n",
        "  installSdxl(env)"
      ],
      "metadata": {
        "id": "C_UxLopJtwU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Adding the refiner and vae. This one took about 14 minutes.\n",
        "#@markdown Skip this step if you don't need these models.\n",
        "!invokeai-model-install --add \"stabilityai/stable-diffusion-xl-refiner-1.0\" --root_dir /content/InvokeAI/invokeai --yes\n",
        "!invokeai-model-install --add \"madebyollin/sdxl-vae-fp16-fix\" --root_dir /content/InvokeAI/invokeai --yes"
      ],
      "metadata": {
        "id": "PauJyLjdbYIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Option 1: Starting the Web UI with ngrok\n",
        "!pip install pyngrok\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "import fileinput\n",
        "import sys\n",
        "\n",
        "Ngrok_token = \"\" #@param {type:\"string\"}\n",
        "#@markdown - Add ngrok token (obtainable from https://ngrok.com)\n",
        "\n",
        "share=''\n",
        "if Ngrok_token!=\"\":\n",
        "  ngrok.kill()\n",
        "  srv=ngrok.connect(9090 , pyngrok_config=conf.PyngrokConfig(auth_token=Ngrok_token),\n",
        "                    bind_tls=True).public_url\n",
        "  print(srv)\n",
        "  !python /content/InvokeAI/scripts/invokeai-web.py --root /content/InvokeAI/invokeai/\n",
        "else:\n",
        "  print('An ngrok token is required. You can get one on https://ngrok.com and paste it into the ngrok_token field.')\n"
      ],
      "metadata": {
        "id": "YgdPgrYKGMYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Option 2: Starting the Web UI with Localtunnel\n",
        "\n",
        "%cd /content/InvokeAI/invokeai/\n",
        "!npm install -g localtunnel\n",
        "\n",
        "#@markdown Copy the IP address shown in the output above the line\n",
        "#@markdown \"your url is: https://some-random-words.loca.lt\"\n",
        "!wget -q -O - ipv4.icanhazip.com\n",
        "\n",
        "#@markdown Wait for the line that says \"Uvicorn running on http://127.0.0.1:9090 (Press CTRL+C to quit)\"\n",
        "\n",
        "#@markdown Click the localtunnel url and paste the IP you copied earlier to the \"Endpoint IP\" text field\n",
        "!lt --port 9090 --local_https False & python /content/InvokeAI/scripts/invokeai-web.py --root /content/InvokeAI/invokeai/\n",
        "\n",
        "#@markdown If the UI shows a red dot that says 'disconnected' when hovered in the upper\n",
        "#@markdown right corner and the Invoke button is disabled, change 'https' to 'http'\n",
        "#@markdown in the browser's address bar and press enter.\n",
        "#@markdown When the page reloads, the UI should work properly.\n"
      ],
      "metadata": {
        "id": "kKYbEWrxuGbE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}